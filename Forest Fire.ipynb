{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388fc04c-0d8d-46b0-9ecb-3c4c5f5b05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e672bcc-88f3-4d64-9752-e355ba2c487e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30250 images belonging to 2 classes.\n",
      "Found 6300 images belonging to 2 classes.\n",
      "Found 6300 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Setup directories\n",
    "train_dir = r'/Users/gurupreetdhande/Summer_internship/fire_detection_app/archive (1)/train'\n",
    "valid_dir = r'/Users/gurupreetdhande/Summer_internship/fire_detection_app/archive (1)/valid'\n",
    "test_dir = r'/Users/gurupreetdhande/Summer_internship/fire_detection_app/archive (1)/test'\n",
    "\n",
    "# Setup ImageDataGenerator for loading images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load images from directiories\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(64, 64), batch_size=32, class_mode='binary')\n",
    "valid_generator = valid_datagen.flow_from_directory(valid_dir, target_size=(64, 64), batch_size=32, class_mode='binary')\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(64, 64), batch_size=32, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061cb00d-6d9a-4460-8a2f-8810a107638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # for data augmentation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog # provides a dialog box to select files.\n",
    "from PIL import Image, ImageTk # ImageTk - converts images for display in Tkinter GUIs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13156bb8-9ee0-4dee-83da-834fbbfd4941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Building a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification: wildfire or no wildfire\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc4f8f0-6d33-473b-bf9e-8c86b5e1dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a14f22-dc6d-448d-957a-02a650348bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 58ms/step - accuracy: 0.8699 - loss: 0.3036 - val_accuracy: 0.9448 - val_loss: 0.1519\n",
      "Epoch 2/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 57ms/step - accuracy: 0.9308 - loss: 0.1883 - val_accuracy: 0.9529 - val_loss: 0.1393\n",
      "Epoch 3/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 59ms/step - accuracy: 0.9385 - loss: 0.1767 - val_accuracy: 0.9165 - val_loss: 0.2024\n",
      "Epoch 4/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 59ms/step - accuracy: 0.9403 - loss: 0.1594 - val_accuracy: 0.9540 - val_loss: 0.1278\n",
      "Epoch 5/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 58ms/step - accuracy: 0.9475 - loss: 0.1373 - val_accuracy: 0.9579 - val_loss: 0.1144\n",
      "Epoch 6/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 58ms/step - accuracy: 0.9551 - loss: 0.1264 - val_accuracy: 0.9546 - val_loss: 0.1259\n",
      "Epoch 7/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 61ms/step - accuracy: 0.9569 - loss: 0.1134 - val_accuracy: 0.9556 - val_loss: 0.1330\n",
      "Epoch 8/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 61ms/step - accuracy: 0.9651 - loss: 0.0985 - val_accuracy: 0.9565 - val_loss: 0.1213\n",
      "Epoch 9/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 61ms/step - accuracy: 0.9678 - loss: 0.0847 - val_accuracy: 0.9595 - val_loss: 0.1147\n",
      "Epoch 10/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 59ms/step - accuracy: 0.9705 - loss: 0.0800 - val_accuracy: 0.9454 - val_loss: 0.1796\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_generator, validation_data=valid_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1cf7ae-e612-4397-903c-e7d9a51bb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification: wildfire or no wildfire\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "467c3b6e-cac0-4e07-9d7b-8db15f9fd63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55180758-21ba-43a2-abbe-e2bd450cd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 58ms/step - accuracy: 0.9012 - loss: 0.2557 - val_accuracy: 0.9263 - val_loss: 0.1975\n",
      "Epoch 2/2\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 59ms/step - accuracy: 0.9312 - loss: 0.1842 - val_accuracy: 0.9425 - val_loss: 0.1552\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "history = model.fit(train_generator, validation_data = valid_generator, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427bd07-83d9-456b-acb2-b07cff7837f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save('ffd_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "235392aa-0539-4a95-abff-4d6fb533eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f28731c-8e9a-49dd-9f32-66a46cef9a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "#Load the model\n",
    "model = load_model('ffd_model.h5')\n",
    "print('Model loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "126d4f7b-ed39-49e2-9a78-8150f0b09265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (10.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b48c8fd3-75fd-412d-bb5e-8da6b7d75363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a1c7e-4d98-45da-a24a-74987e389dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 14:11:00.130 python[31995:6469231] The class 'NSOpenPanel' overrides the method identifier.  This method is implemented by class 'NSWindow'\n"
     ]
    }
   ],
   "source": [
    "# Function to load and predict an image\n",
    "def predict_image():\n",
    "    # Open file dialog to select an image\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        # Display the image in the GUI\n",
    "        img = Image.open(file_path)\n",
    "        img = img.resize((200, 200))\n",
    "        img = ImageTk.PhotoImage(img)    #convert image for tk\n",
    "        image_label.configure(image=img) #update the image in GUI\n",
    "        image_label.image = img\n",
    "\n",
    "        # Preprocess the image for the model\n",
    "        img_for_model = Image.open(file_path).resize((64, 64))\n",
    "        img_array = np.array(img_for_model) / 255.0  # Rescale like during training\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Make a prediction\n",
    "        prediction = model.predict(img_array)[0][0] #extracts the scalar prediction value\n",
    "        result = \"Wildfire\" if prediction > 0.5 else \"No Wildfire\"\n",
    "        result_label.config(text=\"Prediction: \" + result)\n",
    "\n",
    "# Setting up the GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Forest Fire Detection\")\n",
    "root.geometry(\"400x400\")\n",
    "\n",
    "# Add widgets\n",
    "btn = tk.Button(root, text=\"Upload Image\", command=predict_image) #button triggers the predict_image() function when clicked\n",
    "btn.pack(pady=20)\n",
    "\n",
    "#Placeholder for displaying the selected image\n",
    "image_label = tk.Label(root)\n",
    "image_label.pack()\n",
    "\n",
    "#Label to display the prediction result\n",
    "result_label = tk.Label(root, text=\"Prediction: \", font=(\"Helvetica\", 16))\n",
    "result_label.pack(pady=20)\n",
    "\n",
    "#Starts the Tkinter event loop, keeping the GUI active until manually closed\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277c42d-715a-4ed6-8b1c-71152eb58fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7066e-667e-43bb-9b41-3ef91170b347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
